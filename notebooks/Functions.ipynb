{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions that are used in the rest of the repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "default_path= #the path\n",
    "os.chdir(default_path)\n",
    "df_address=\"single_df_filled\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fill_month function adds a row for each month for each employee where it's missing. Then it populates these new rows using ffill and bfill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_month(df1, start_date, end_date, freq):\n",
    "    month_dict={'': 0,\n",
    "     'jan': 1,\n",
    "     'feb': 2,\n",
    "     'mar': 3,\n",
    "     'apr': 4,\n",
    "     'mai': 5,\n",
    "     'jun': 6,\n",
    "     'jul': 7,\n",
    "     'aug': 8,\n",
    "     'sep': 9,\n",
    "     'okt': 10,\n",
    "     'nov': 11,\n",
    "     'des': 12}\n",
    "     \n",
    "    dates = df1.date.astype(str).str.split('.',expand=True)\n",
    "    dates.iloc[:,-2]= dates.iloc[:,-2].replace(month_dict)\n",
    "    df1['date'] = dates.iloc[:,-2].astype(str) + '/' + dates.iloc[:,-1].astype(str).map(lambda x: str(x)[-2:])\n",
    "    \n",
    "    #___reformat the date columns\n",
    "    df1['date'] = pd.to_datetime(df1['date'], format='%m/%y')\n",
    "    \n",
    "    #create all months in the time range\n",
    "    all_months=pd.date_range(start=start_date, end=end_date, freq=freq)\n",
    "    id_list=list(df1['id'].unique())\n",
    "    \n",
    "    #ful index with all combinations of date and id\n",
    "    full_index = pd.MultiIndex.from_product([id_list, all_months],names=['id', 'date'])\n",
    "    #create a new df, filling the nan values based on the previous records \n",
    "    #!!!important that set_index and full index have the same rekkefÃ¸lge\n",
    "    df_t1=df1.set_index(['id','date']).reindex(full_index).reset_index()\n",
    "    return df_t1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-coding the categorical variables\n",
    "I'm using one hot coding to recode the categorical variables. \n",
    "We'll be using the functions in the RFC notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_coding(df_address, drop_cols):\n",
    "    df= pd.read_csv(('data/processed/' + df_address + '.csv'))\n",
    "    df_ready= df.drop(columns= drop_cols) # removing attend and absence cause we have the average\n",
    "    \n",
    "    #ONE HOT CODING\n",
    "    cat_vars=['host_country', 'nation', 'host_country_man', 'nation_man']  \n",
    "    \n",
    "    for col in cat_vars:\n",
    "        #create other variable for less then 0.5 percent\n",
    "        series = pd.value_counts(df_ready[col])\n",
    "        mask = (series/series.sum() * 100).lt(0.55) #change lt() to change the other criteria\n",
    "        # To replace df['column'] use np.where I.e \n",
    "        df_ready[col] = np.where(df_ready[col].isin(series[mask].index),'Other',df_ready[col])\n",
    "    \n",
    "    cat_vars=['host_country', 'nation', 'host_country_man', 'nation_man', 'BA', 'network', 'BA_man', 'network_man']  \n",
    "    df_coded = pd.get_dummies(df_ready, prefix_sep=\"_\", columns=cat_vars)\n",
    "    #df_coded.drop(columns=cat_vars, axis=1, inplace=True)\n",
    "    df_coded.columns.values\n",
    "    \n",
    "    df_coded=df_coded.astype('float64')\n",
    "    \n",
    "    return df_coded\n",
    "\n",
    "\n",
    "drop_cols= ['date', 'name', 'home_country', 'home_country_man', 'is_norsk', 'in_norge', 'is_norsk_man',\n",
    "            'in_norge_man', 'name_man', 'is_manager_man', 'manager_id', 'manager_id_man']\n",
    "\n",
    "df_coded= data_coding(df_address, drop_cols)\n",
    "df_coded.to_csv(('data/processed/' + df_address + '_coded.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATE TRAIN TEST SPLITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y is the column name #identify is the id column name\n",
    "def test_train_manual(main_df, y, identif):\n",
    "\n",
    "    #divide the dataset into leavers and non-leavers\n",
    "    y_1=main_df[main_df[y]==1]\n",
    "    y_0=main_df[main_df[y]==0]\n",
    "    #create list of unique ids to make sure data for the same id is not found in the both dataset\n",
    "    y_1_id= list(y_1[identif].unique()) #list of ids for nonleavers\n",
    "    y_0_id= list(y_0[identif].unique())\n",
    "    \n",
    "    #split the leaver and nonleaver ids into 25% and put them together for the test set\n",
    "    test_id= random.sample(y_1_id, (len(y_1_id)//4)) + random.sample(y_0_id, (len(y_0_id)//4))\n",
    "    train_id= [x for x in list(main_df[identif].unique()) if x not in test_id] #remaining ids is the train set\n",
    "    \n",
    "    #test and train dfs based on the ids selected\n",
    "    test_df=main_df[main_df[identif].isin(test_id)]\n",
    "    train_df=main_df[main_df[identif].isin(train_id)]\n",
    "    \n",
    "    #split X and y\n",
    "    test_df.drop(columns=identif, inplace=True)\n",
    "    train_df.drop(columns=identif, inplace=True)\n",
    "    \n",
    "    features=test_df.columns.values.tolist() #features are the same in both\n",
    "    y=[y]\n",
    "    X=[i for i in features if i not in y]\n",
    "    \n",
    "    X_test=test_df[X]\n",
    "    X_train=train_df[X]\n",
    "    y_test=test_df[y].values.ravel()\n",
    "    y_train=train_df[y].values.ravel()\n",
    "    return X_test, X_train, y_test, y_train\n",
    "\n",
    "\n",
    "X_test, X_train, y_test, y_train = test_train_manual(df_coded, 'Leavers', 'id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select features\n",
    "We'll run a RFC to choose the most important features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features(X_test, X_train, y_test, y_train):\n",
    "    rf_init = RandomForestClassifier(n_estimators= 400, random_state=42)\n",
    "    rf_init.fit(X_train, y_train)\n",
    "    \n",
    "    print(classification_report(y_test, rf_init.predict(X_test)))\n",
    "    \n",
    "    #________________feature importance\n",
    "    cols=X_test.columns.values.tolist()\n",
    "    # List of tuples with variable and importance\n",
    "    feature_importances = [(feature, round(importance, 2)) for feature,\n",
    "                           importance in zip(cols, list(rf_init.feature_importances_))]\n",
    "    \n",
    "    # Sort the feature importances by most important first\n",
    "    feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "    print(feature_importances)\n",
    "\n",
    "    # Extract the names of the most important features based on the number of imp features\n",
    "    important_feature_names = [feature[0]  for feature in feature_importances if feature [1]>0]\n",
    "    \n",
    "    #____________Create training and testing sets with only the important features\n",
    "    imp_X_train = X_train[important_feature_names]\n",
    "    imp_X_test = X_test[important_feature_names]\n",
    "    # Sanity check on operations\n",
    "    print('Important train features shape:', imp_X_train.shape)\n",
    "    print('Important test features shape:', imp_X_test.shape)\n",
    "    \n",
    "    return imp_X_train, imp_X_test, feature_importances\n",
    "\n",
    "imp_X_train, imp_X_test, feature_importances = select_features(X_test, X_train, y_test, y_train)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
